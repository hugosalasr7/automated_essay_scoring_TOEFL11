{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Data wrangling\n",
    "### Author: Hugo Salas; Dataset: TOEFL11\n",
    "\n",
    "This notebook uses the essays from the TOEFL11 dataset and its additional features. It creates additional features and cleans the free text (essay) data. Specifically:\n",
    "- It creates measures of essay length, unique words used, lexical diversity, mispelled words\n",
    "- One-hot encodes categorical features (language, prompt, score)\n",
    "- Tokenizes each essay\n",
    "- Changes mispelled words by correctly spelled words\n",
    "- Creates TF-IDF\n",
    "\n",
    "\n",
    "## 1. Import modules and data\n",
    "#### 1a) Relevant modules and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import nltk \n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Globals\n",
    "PATH_DATA = '../data/text/'\n",
    "PATH_RESP = f'{PATH_DATA}/responses/'\n",
    "RESP_JSON_OR = 'toefl11_resp_or.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b) Data in CSV format: info of each essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12100, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Language</th>\n",
       "      <th>Score Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.txt</td>\n",
       "      <td>P6</td>\n",
       "      <td>KOR</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278.txt</td>\n",
       "      <td>P6</td>\n",
       "      <td>DEU</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>348.txt</td>\n",
       "      <td>P1</td>\n",
       "      <td>TUR</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666.txt</td>\n",
       "      <td>P2</td>\n",
       "      <td>ZHO</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>733.txt</td>\n",
       "      <td>P6</td>\n",
       "      <td>TEL</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filename Prompt Language Score Level\n",
       "0   88.txt     P6      KOR        high\n",
       "1  278.txt     P6      DEU      medium\n",
       "2  348.txt     P1      TUR        high\n",
       "3  666.txt     P2      ZHO      medium\n",
       "4  733.txt     P6      TEL      medium"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import CSV with some features\n",
    "toefl11_df = pd.read_csv(f'{PATH_DATA}index.csv')\n",
    "print(toefl11_df.shape)\n",
    "toefl11_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c) Data in txt: actual essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some people might think that traveling in a group led by a tour guide is a good way .\n",
      "But , a group \n"
     ]
    }
   ],
   "source": [
    "# Import essays (free text) and put on dict\n",
    "toefl11_responses_dict = {}\n",
    "txt_file_lst = toefl11_df['Filename'].values\n",
    "\n",
    "for txt_file in txt_file_lst:\n",
    "    with open(f'{PATH_RESP}/tokenized/{txt_file}') as f:\n",
    "        text = f.read()\n",
    "        toefl11_responses_dict[txt_file] = text\n",
    "\n",
    "# Save as JSON for future use\n",
    "with open(f'{PATH_RESP}/toefl11_resp_tok.json', 'w') as fp:\n",
    "    json.dump(toefl11_responses_dict, fp)\n",
    "\n",
    "print(toefl11_responses_dict['88.txt'][:100]) #Print first 100 characters of one essay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute relevant features\n",
    "#### 2a. Misspelled words, essay word length, unique tokens per essay, tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterated over 0 essays\n",
      "Iterated over 1000 essays\n",
      "Iterated over 2000 essays\n",
      "Iterated over 3000 essays\n",
      "Iterated over 4000 essays\n",
      "Iterated over 5000 essays\n",
      "Iterated over 6000 essays\n",
      "Iterated over 7000 essays\n",
      "Iterated over 8000 essays\n",
      "Iterated over 9000 essays\n",
      "Iterated over 10000 essays\n",
      "Iterated over 11000 essays\n",
      "Iterated over 12000 essays\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "contractions = [\"'ll\", \"'ve\", \"'d\", \"'s\", \"n't\", \"'re\", \"'t\"]\n",
    "acronyms = [\"tv\", \"tvs\", \"gps\"]\n",
    "\n",
    "essay_len = []\n",
    "uniq_token_len = []\n",
    "mispelled_wds = []\n",
    "toefl11_tokens_dict = {}\n",
    "toefl11_tokens_corrected_dict = {}\n",
    "\n",
    "for i, txt_file in enumerate(txt_file_lst):\n",
    "    essay = toefl11_responses_dict[txt_file]\n",
    "    # Tokenize\n",
    "    token_lst = nltk.tokenize.word_tokenize(essay)\n",
    "    # Grab relevant length indicators\n",
    "    essay_len.append( len(token_lst) ) # Essay length\n",
    "    uniq_token_len.append( len(set(token_lst)) ) # Unique tokens\n",
    "\n",
    "    no_cont_acr = []\n",
    "    token_lst_corrected = []\n",
    "    for word in token_lst:\n",
    "        # List without acronyms\n",
    "        if word.lower() not in acronyms + contractions:\n",
    "            no_cont_acr.append(word)\n",
    "        # List with all tokens corrected\n",
    "        if spell.unknown([word])==set() or word.lower() in acronyms + contractions:\n",
    "            token_lst_corrected.append( word )\n",
    "        else:\n",
    "            token_lst_corrected.append( spell.correction(word) )\n",
    "\n",
    "    # Get all mispelled words (don't include contractions or acronyms)\n",
    "    misspelled = spell.unknown(no_cont_acr)\n",
    "    mispelled_wds.append(len(misspelled))\n",
    "\n",
    "    # Save tokenized versions of the essay\n",
    "    toefl11_tokens_dict[txt_file] = token_lst\n",
    "    toefl11_tokens_corrected_dict[txt_file] = token_lst_corrected\n",
    "\n",
    "    if i%1000==0:\n",
    "        print(f\"Iterated over {i} essays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as JSON files for future use\n",
    "with open(f'{PATH_RESP}/toefl11_tokenized_tok.json', 'w') as fp:\n",
    "    json.dump(toefl11_tokens_dict, fp)\n",
    "\n",
    "with open(f'{PATH_RESP}/toefl11_tokenized_corrected_tok.json', 'w') as fp:\n",
    "    json.dump(toefl11_tokens_corrected_dict, fp)\n",
    "\n",
    "# Add vars to DF\n",
    "toefl11_df['Essay length'] = essay_len\n",
    "toefl11_df['Unique tokens'] = uniq_token_len\n",
    "toefl11_df['Mispelled words'] = mispelled_wds\n",
    "\n",
    "#Delete objects we don't need\n",
    "del token_lst, token_lst_corrected, essay, essay_len, uniq_token_len, mispelled_wds, text, no_cont_acr, spell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Calculate lexical diversity indicators. Specifically:\n",
    "- words: number of words (w)\n",
    "- terms: number of unique terms (t)\n",
    "- ttr: type-token ratio computed as t / w (Chotlos 1944, Templin 1957)\n",
    "- rttr: root TTR computed as t / sqrt(w) (Guiraud 1954, 1960)\n",
    "- cttr: corrected TTR computed as t / sqrt(2w) (Carrol 1964)\n",
    "- Herdan: log(t) / log(w) (Herdan 1960, 1964)\n",
    "- Summer: log(log(t)) / log(log(w)) Summer (1966)\n",
    "- Dugast: (log(w) ** 2) / (log(w) - log(t) Dugast (1978)\n",
    "- Maas: (log(w) - log(t)) / (log(w) ** 2) Maas (1972)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "toefl11_df['ttr'] = toefl11_df['Unique tokens'] / toefl11_df['Essay length']\n",
    "toefl11_df['rttr'] = toefl11_df['Unique tokens'] / np.sqrt(toefl11_df['Essay length'])\n",
    "toefl11_df['cttr'] = toefl11_df['Unique tokens'] / np.sqrt(2*toefl11_df['Essay length'])\n",
    "toefl11_df['herdan'] = np.log(toefl11_df['Unique tokens']) / np.log(toefl11_df['Essay length'])\n",
    "toefl11_df['summer'] = np.log(np.log(toefl11_df['Unique tokens'])) / np.log(np.log(toefl11_df['Essay length']))\n",
    "toefl11_df['dugast'] = np.log(toefl11_df['Unique tokens']**2) / (np.log(toefl11_df['Essay length']) - np.log(toefl11_df['Unique tokens']))\n",
    "toefl11_df['maas'] =  (np.log(toefl11_df['Essay length']) - np.log(toefl11_df['Unique tokens'])) / np.log(toefl11_df['Essay length']**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c. One-hot encoding of categorical vars (language, prompt and score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, pref in [('Score Level', 'score'), ('Language', 'lang'), ('Prompt', 'prompt')]:\n",
    "    toefl11_df = toefl11_df.merge(pd.get_dummies(toefl11_df[col], prefix=pref), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2d. Create quantiles for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ttr</th>\n",
       "      <th>rttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>herdan</th>\n",
       "      <th>summer</th>\n",
       "      <th>dugast</th>\n",
       "      <th>maas</th>\n",
       "      <th>score_high</th>\n",
       "      <th>score_low</th>\n",
       "      <th>score_medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ttr</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.334179</td>\n",
       "      <td>0.334179</td>\n",
       "      <td>0.951878</td>\n",
       "      <td>0.884190</td>\n",
       "      <td>0.875594</td>\n",
       "      <td>-0.951878</td>\n",
       "      <td>-0.028126</td>\n",
       "      <td>0.239812</td>\n",
       "      <td>-0.123693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rttr</th>\n",
       "      <td>0.334179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.596301</td>\n",
       "      <td>0.717627</td>\n",
       "      <td>0.401372</td>\n",
       "      <td>-0.596301</td>\n",
       "      <td>0.430634</td>\n",
       "      <td>-0.392711</td>\n",
       "      <td>-0.164988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cttr</th>\n",
       "      <td>0.334179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.596301</td>\n",
       "      <td>0.717627</td>\n",
       "      <td>0.401372</td>\n",
       "      <td>-0.596301</td>\n",
       "      <td>0.430634</td>\n",
       "      <td>-0.392711</td>\n",
       "      <td>-0.164988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>herdan</th>\n",
       "      <td>0.951878</td>\n",
       "      <td>0.596301</td>\n",
       "      <td>0.596301</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983942</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.108371</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>-0.147963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>summer</th>\n",
       "      <td>0.884190</td>\n",
       "      <td>0.717627</td>\n",
       "      <td>0.717627</td>\n",
       "      <td>0.983942</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820049</td>\n",
       "      <td>-0.983942</td>\n",
       "      <td>0.183049</td>\n",
       "      <td>-0.032905</td>\n",
       "      <td>-0.154283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dugast</th>\n",
       "      <td>0.875594</td>\n",
       "      <td>0.401372</td>\n",
       "      <td>0.401372</td>\n",
       "      <td>0.856242</td>\n",
       "      <td>0.820049</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.856242</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>0.123963</td>\n",
       "      <td>-0.135102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maas</th>\n",
       "      <td>-0.951878</td>\n",
       "      <td>-0.596301</td>\n",
       "      <td>-0.596301</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.983942</td>\n",
       "      <td>-0.856242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.108371</td>\n",
       "      <td>-0.070700</td>\n",
       "      <td>0.147963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_high</th>\n",
       "      <td>-0.028126</td>\n",
       "      <td>0.430634</td>\n",
       "      <td>0.430634</td>\n",
       "      <td>0.108371</td>\n",
       "      <td>0.183049</td>\n",
       "      <td>0.060051</td>\n",
       "      <td>-0.108371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.256323</td>\n",
       "      <td>-0.794776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_low</th>\n",
       "      <td>0.239812</td>\n",
       "      <td>-0.392711</td>\n",
       "      <td>-0.392711</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>-0.032905</td>\n",
       "      <td>0.123963</td>\n",
       "      <td>-0.070700</td>\n",
       "      <td>-0.256323</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.382907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_medium</th>\n",
       "      <td>-0.123693</td>\n",
       "      <td>-0.164988</td>\n",
       "      <td>-0.164988</td>\n",
       "      <td>-0.147963</td>\n",
       "      <td>-0.154283</td>\n",
       "      <td>-0.135102</td>\n",
       "      <td>0.147963</td>\n",
       "      <td>-0.794776</td>\n",
       "      <td>-0.382907</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ttr      rttr      cttr    herdan    summer    dugast  \\\n",
       "ttr           1.000000  0.334179  0.334179  0.951878  0.884190  0.875594   \n",
       "rttr          0.334179  1.000000  1.000000  0.596301  0.717627  0.401372   \n",
       "cttr          0.334179  1.000000  1.000000  0.596301  0.717627  0.401372   \n",
       "herdan        0.951878  0.596301  0.596301  1.000000  0.983942  0.856242   \n",
       "summer        0.884190  0.717627  0.717627  0.983942  1.000000  0.820049   \n",
       "dugast        0.875594  0.401372  0.401372  0.856242  0.820049  1.000000   \n",
       "maas         -0.951878 -0.596301 -0.596301 -1.000000 -0.983942 -0.856242   \n",
       "score_high   -0.028126  0.430634  0.430634  0.108371  0.183049  0.060051   \n",
       "score_low     0.239812 -0.392711 -0.392711  0.070700 -0.032905  0.123963   \n",
       "score_medium -0.123693 -0.164988 -0.164988 -0.147963 -0.154283 -0.135102   \n",
       "\n",
       "                  maas  score_high  score_low  score_medium  \n",
       "ttr          -0.951878   -0.028126   0.239812     -0.123693  \n",
       "rttr         -0.596301    0.430634  -0.392711     -0.164988  \n",
       "cttr         -0.596301    0.430634  -0.392711     -0.164988  \n",
       "herdan       -1.000000    0.108371   0.070700     -0.147963  \n",
       "summer       -0.983942    0.183049  -0.032905     -0.154283  \n",
       "dugast       -0.856242    0.060051   0.123963     -0.135102  \n",
       "maas          1.000000   -0.108371  -0.070700      0.147963  \n",
       "score_high   -0.108371    1.000000  -0.256323     -0.794776  \n",
       "score_low    -0.070700   -0.256323   1.000000     -0.382907  \n",
       "score_medium  0.147963   -0.794776  -0.382907      1.000000  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it seems like the lexical diversity measure that is more correlated with the scores is ttr and rttr,\n",
    "# we will only analyze ttr which also happens to be the most simple\n",
    "toefl11_df[['ttr', 'rttr', 'cttr', 'herdan', 'summer', 'dugast', 'maas', 'score_high', 'score_low', 'score_medium']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "toefl11_df['mispelled_ratio'] = toefl11_df['Mispelled words']/toefl11_df['Essay length']\n",
    "for col, pref in [('Essay length', 'essaylen_quint'), ('Mispelled words', 'misp_quint'), ('rttr', 'rttr_quint'), ('Unique tokens', 'un_tok_quint'), ('mispelled_ratio', 'misp_ratio_quint')] :\n",
    "    toefl11_df = toefl11_df.merge(pd.get_dummies( pd.qcut(toefl11_df['rttr'], 5, labels=False), prefix = 'rttr_quint'), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2e. Save DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "toefl11_df.to_pickle(f'{PATH_RESP}/toefl11_DF.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Filename', 'Prompt', 'Language', 'Score Level', 'Essay length',\n",
       "       'Unique tokens', 'Mispelled words', 'ttr', 'rttr', 'cttr', 'herdan',\n",
       "       'summer', 'dugast', 'maas', 'score_high', 'score_low', 'score_medium',\n",
       "       'lang_ARA', 'lang_DEU', 'lang_FRA', 'lang_HIN', 'lang_ITA', 'lang_JPN',\n",
       "       'lang_KOR', 'lang_SPA', 'lang_TEL', 'lang_TUR', 'lang_ZHO', 'prompt_P1',\n",
       "       'prompt_P2', 'prompt_P3', 'prompt_P4', 'prompt_P5', 'prompt_P6',\n",
       "       'prompt_P7', 'prompt_P8', 'rttr_quint_0_x', 'rttr_quint_1_x',\n",
       "       'rttr_quint_2_x', 'rttr_quint_3_x', 'rttr_quint_4_x', 'rttr_quint_0_y',\n",
       "       'rttr_quint_1_y', 'rttr_quint_2_y', 'rttr_quint_3_y', 'rttr_quint_4_y',\n",
       "       'rttr_quint_0', 'rttr_quint_1', 'rttr_quint_2', 'rttr_quint_3',\n",
       "       'rttr_quint_4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toefl11_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Language</th>\n",
       "      <th>Score Level</th>\n",
       "      <th>Essay length</th>\n",
       "      <th>Unique tokens</th>\n",
       "      <th>Mispelled words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>rttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>...</th>\n",
       "      <th>rttr_quint_0_y</th>\n",
       "      <th>rttr_quint_1_y</th>\n",
       "      <th>rttr_quint_2_y</th>\n",
       "      <th>rttr_quint_3_y</th>\n",
       "      <th>rttr_quint_4_y</th>\n",
       "      <th>rttr_quint_0</th>\n",
       "      <th>rttr_quint_1</th>\n",
       "      <th>rttr_quint_2</th>\n",
       "      <th>rttr_quint_3</th>\n",
       "      <th>rttr_quint_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>88.txt</td>\n",
       "      <td>P6</td>\n",
       "      <td>KOR</td>\n",
       "      <td>high</td>\n",
       "      <td>416</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.391827</td>\n",
       "      <td>7.991733</td>\n",
       "      <td>5.651008</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>278.txt</td>\n",
       "      <td>P6</td>\n",
       "      <td>DEU</td>\n",
       "      <td>medium</td>\n",
       "      <td>339</td>\n",
       "      <td>129</td>\n",
       "      <td>3</td>\n",
       "      <td>0.380531</td>\n",
       "      <td>7.006318</td>\n",
       "      <td>4.954215</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>348.txt</td>\n",
       "      <td>P1</td>\n",
       "      <td>TUR</td>\n",
       "      <td>high</td>\n",
       "      <td>396</td>\n",
       "      <td>195</td>\n",
       "      <td>5</td>\n",
       "      <td>0.492424</td>\n",
       "      <td>9.799119</td>\n",
       "      <td>6.929023</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>666.txt</td>\n",
       "      <td>P2</td>\n",
       "      <td>ZHO</td>\n",
       "      <td>medium</td>\n",
       "      <td>402</td>\n",
       "      <td>166</td>\n",
       "      <td>2</td>\n",
       "      <td>0.412935</td>\n",
       "      <td>8.279327</td>\n",
       "      <td>5.854369</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>733.txt</td>\n",
       "      <td>P6</td>\n",
       "      <td>TEL</td>\n",
       "      <td>medium</td>\n",
       "      <td>362</td>\n",
       "      <td>149</td>\n",
       "      <td>8</td>\n",
       "      <td>0.411602</td>\n",
       "      <td>7.831266</td>\n",
       "      <td>5.537541</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Filename Prompt Language Score Level  Essay length  Unique tokens  \\\n",
       "0   88.txt     P6      KOR        high           416            163   \n",
       "1  278.txt     P6      DEU      medium           339            129   \n",
       "2  348.txt     P1      TUR        high           396            195   \n",
       "3  666.txt     P2      ZHO      medium           402            166   \n",
       "4  733.txt     P6      TEL      medium           362            149   \n",
       "\n",
       "   Mispelled words       ttr      rttr      cttr  ...  rttr_quint_0_y  \\\n",
       "0                0  0.391827  7.991733  5.651008  ...               0   \n",
       "1                3  0.380531  7.006318  4.954215  ...               1   \n",
       "2                5  0.492424  9.799119  6.929023  ...               0   \n",
       "3                2  0.412935  8.279327  5.854369  ...               0   \n",
       "4                8  0.411602  7.831266  5.537541  ...               0   \n",
       "\n",
       "   rttr_quint_1_y  rttr_quint_2_y  rttr_quint_3_y  rttr_quint_4_y  \\\n",
       "0               1               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               1   \n",
       "3               0               1               0               0   \n",
       "4               1               0               0               0   \n",
       "\n",
       "   rttr_quint_0  rttr_quint_1  rttr_quint_2  rttr_quint_3  rttr_quint_4  \n",
       "0             0             1             0             0             0  \n",
       "1             1             0             0             0             0  \n",
       "2             0             0             0             0             1  \n",
       "3             0             0             1             0             0  \n",
       "4             0             1             0             0             0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toefl11_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "#### Will do for 1000 and 1500 features and 1-3 ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tfidf(max_features, max_df = 0.9, min_df = 0.01, ngram_range = (1,1)):\n",
    "    cvect = CountVectorizer(lowercase=True, max_features=max_features, max_df=max_df, min_df=min_df , ngram_range= ngram_range)\n",
    "    tfIdfTransformer = TfidfTransformer(use_idf=True)\n",
    "    cvect_df = cvect.fit_transform([\" \".join(toefl11_tokens_corrected_dict[txt_file]) for txt_file in txt_file_lst])\n",
    "    cvect_tfidf_df = tfIdfTransformer.fit_transform(cvect_df)\n",
    "    cvect_tfidf_df = pd.DataFrame(cvect_tfidf_df.toarray(),\n",
    "                                columns=cvect.get_feature_names())\n",
    "    return cvect_tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and save dataframes\n",
    "tfidf = {}\n",
    "for name, max, ngr in [(\"tfidf_1k\", 1000, (1,1)), \n",
    "                       (\"tfidf_15k\", 1500, (1,1)), \n",
    "                       (\"tfidf_1k_3ngr\", 1000, (1,3)), \n",
    "                       (\"tfidf_15k_3ngr\", 1500, (1,3))]:\n",
    "                       \n",
    "    tfidf[name] = construct_tfidf(max, ngram_range = ngr)\n",
    "    tfidf[name].to_pickle(f'{PATH_RESP}/toefl11_{name}.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39af225174ef6eadf8966b2d9d8945b95dfd3f8431db13d1fd2e5fe3240483e4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
